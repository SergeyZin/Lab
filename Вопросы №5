1. Как связаны главные компоненты с исходными данными?
  Метод главных компонент (PCA) - это способ уменьшения размерности данных путем создания новых признаков, являющихся линейными комбинациями исходных признаков. Эти новые признаки, называемые главными компонентами, расположены вдоль направлений максимальной дисперсии в данных и упорядочены по убыванию объясняемой дисперсии. Главные компоненты ортогональны друг другу, что позволяет избежать избыточности информации. Таким образом, PCA сохраняет наиболее важную информацию из исходных данных в меньшем числе новых признаков, облегчая дальнейшую обработку и визуализацию данных.

2. Сделайте грубую оценку сжатия данных, если исходная матрица имела размерность (4250, 7), а при восстановлении используются три главные компоненты.
  Для оценки степени сжатия данных после применения PCA нужно рассчитать отношение размерности восстановленных данных к исходной размерности. Изначально данные имели размерность (4250, 7). После восстановления с использованием трех главных компонент размерность стала (4250, 3). Отношение новой размерности к исходной равно 3/7 ≈ 0,43. Это означает, что размерность данных уменьшилась примерно на 57%, или, другими словами, данные были сжаты приблизительно на 57%.

3. Сгенерируйте данные в виде эллипса с центром в точке (1.5, -2.5), радиусами (3, 2.5), углом 65 и количеством точек 1100.
  Оцените собственные вектора, собственные значения, максимальные и минимальные значения в пространстве главных компонент.
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA
  center = (1.5, -2.5)
  radii = (3.0, 2.5)
  angle = 65 * np.pi / 180  # Угол в радианах
  num_points = 1100
  t = np.linspace(0, 2 * np.pi, num_points)
  x = center[0] + radii[0] * np.cos(t) * np.cos(angle) - radii[1] * np.sin(t) * np.sin(angle)
  y = center[1] + radii[0] * np.cos(t) * np.sin(angle) + radii[1] * np.sin(t) * np.cos(angle)
  data = np.column_stack((x, y))
  pca = PCA()
  pca.fit(data)
  eigenvectors = pca.components_
  eigenvalues = pca.explained_variance_
  data_transformed = pca.transform(data)
  max_values = np.max(data_transformed, axis=0)
  min_values = np.min(data_transformed, axis=0)
  print("Собственные вектора:")
  print(eigenvectors)
  print("\nСобственные значения:")
  print(eigenvalues)
  print("\nМаксимальные значения в пространстве главных компонент:")
  print(max_values)
  print("\nМинимальные значения в пространстве главных компонент:")
  print(min_values)
  plt.scatter(x, y)
  plt.title("Данные в виде эллипса")
  plt.show()
  Порядок выполнения пошагово:
  Импортируются необходимые библиотеки: NumPy для численных вычислений, Matplotlib для визуализации и PCA из scikit-learn для вычисления главных компонент.
  Задаются параметры эллипса: центр, радиусы, угол наклона и количество точек.
  Генерируются данные в виде эллипса с использованием параметрического уравнения эллипса.
  Создается экземпляр PCA и вычисляются главные компоненты для сгенерированных данных.
  Извлекаются собственные вектора, собственные значения, а также максимальные и минимальные значения в пространстве главных компонент.
  Выводятся на экран собственные вектора, собственные значения, максимальные и минимальные значения в пространстве главных компонент.
  Визуализируются исходные данные в виде эллипса с помощью Matplotlib.

4. Для набора данных Cars проанализируйте веса главных компонент при использовании числовых признаков. Какой из параметров вносит наименьший вклад в первую главную компоненту?
  Для анализа весов главных компонент для набора данных Cars с использованием числовых признаков можно использовать следующий код на Python:
  import pandas as pd
  from sklearn.decomposition import PCA
  cars_data = pd.read_csv("cars.csv")
  pca = PCA(n_components=5)
  pca_result = pca.fit_transform(cars_data)
  weights = pca.components_
  weights_first_component = weights[0]
  smallest_weight_param = cars_data.columns[np.argmin(np.abs(weights_first_component))]
  print("Параметр с наименьшим вкладом в первую главную компоненту:", smallest_weight_param) 
